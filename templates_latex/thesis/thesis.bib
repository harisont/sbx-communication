
@inproceedings{borin_korp_2012,
	address = {Istanbul, Turkey},
	title = {Korp: {The} corpus infrastructure of {Språkbanken}},
	url = {https://gup.ub.gu.se/publication/156080},
	abstract = {We present Korp, the corpus infrastructure of Språkbanken (the Swedish Language Bank). The infrastructure consists of three main components: the Korp corpus pipeline, the Korp backend, and the Korp frontend. The Korp corpus pipeline is used for importing corpora, annotating them, and then exporting the annotated corpora into different formats. An essential feature of the pipeline is the ability to leave existing annotations untouched, both structural and word level annotations, and to use the existing annotations as the foundation of other annotations. The Korp backend consists of a set of REST-based web services for searching in and retrieving information about the corpora. Finally, the Korp frontend is a graphical search interface that interacts with the Korp backend. The interface has been inspired by corpus search interfaces such as SketchEngine, Glossa, and DeepDict, and it uses State Chart XML (SCXML) in order to enable users to bookmark interaction states. We give a functional and technical overview of the three components, followed by a discussion of planned future work.},
	language = {en},
	booktitle = {Proceedings of {LREC} 2012},
	publisher = {European Language Resources Association},
	author = {Borin, Lars and Forsberg, Markus and Roxendal, Johan},
	year = {2012},
	pages = {474--478},
	file = {Borin et al. - Korp – the corpus infrastructure of Språkbanken.pdf:/home/stian/Zotero/storage/KET4N4YV/Borin et al. - Korp – the corpus infrastructure of Språkbanken.pdf:application/pdf},
}

@article{beel_research-paper_2016,
	title = {Research-paper recommender systems: {A} literature survey},
	volume = {17},
	issn = {1432-5012, 1432-1300},
	shorttitle = {Research-paper recommender systems},
	url = {http://link.springer.com/10.1007/s00799-015-0156-0},
	doi = {10.1007/s00799-015-0156-0},
	language = {en},
	number = {4},
	urldate = {2022-04-30},
	journal = {International Journal on Digital Libraries},
	author = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
	month = nov,
	year = {2016},
	pages = {305--338},
	file = {Submitted Version:/home/stian/Zotero/storage/3CYHHQQ7/Beel et al. - 2016 - Research-paper recommender systems a literature s.pdf:application/pdf},
}

@inproceedings{borin_sparv_2016,
	address = {Umeå, Sweden},
	title = {Sparv: {Språkbanken}’s corpus annotation pipeline infrastructure},
	url = {https://gup.ub.gu.se/publication/246053},
	abstract = {Sparv is Språkbanken’s corpus annotation pipeline infrastructure. The easiest way to use the pipeline is from its web interface with a plain text document. The pipeline uses in-house and external tools on the text to segment it into sentences and paragraphs, tokenise, tag parts-of-speech, look up in dictionaries and analyse compounds. The pipeline can also be run using a web API with XML results, and it is run locally at Språkbanken to prepare the documents in Korp, our corpus search tool. While the most sophisticated support is for modern Swedish, the pipeline supports 15 languages.},
	language = {en},
	booktitle = {Proceedings of {SLTC} 2016},
	publisher = {Umeå University},
	author = {Borin, Lars and Forsberg, Markus and Hammarstedt, Martin and Rosén, Dan and Schäfer, Roland and Schumacher, Anne},
	year = {2016},
	file = {Borin et al. - Sparv Språkbanken’s corpus annotation pipeline in.pdf:/home/stian/Zotero/storage/RUI5D2GC/Borin et al. - Sparv Språkbanken’s corpus annotation pipeline in.pdf:application/pdf},
}

@inproceedings{dridan_tokenization_2012,
	address = {Jeju Island, Korea},
	title = {Tokenization: {Returning} to a long solved problem -- a survey, contrastive experiment, recommendations, and toolkit},
	shorttitle = {Tokenization},
	url = {https://aclanthology.org/P12-2074},
	urldate = {2022-04-30},
	booktitle = {Proceedings of the 50th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dridan, Rebecca and Oepen, Stephan},
	month = jul,
	year = {2012},
	pages = {378--382},
	file = {Full Text PDF:/home/stian/Zotero/storage/R35QTRH3/Dridan and Oepen - 2012 - Tokenization Returning to a Long Solved Problem —.pdf:application/pdf},
}

@inproceedings{frunza_trainable_2008,
	address = {Marrakech, Morocco},
	title = {A {Trainable} {Tokenizer}, solution for multilingual texts and compound expression tokenization},
	isbn = {2-9517408-4-0},
	url = {http://www.lrec-conf.org/proceedings/lrec2008/},
	abstract = {Tokenization is one of the initial steps done for almost any text processing task. It is not particularly recognized as a challenging task for English monolingual systems but it rapidly increases in complexity for systems that apply it for different languages. This article proposes a supervised learning approach to perform the tokenization task. The method presented in this article is based on character transitions representation, a representation that allows compound expressions to be recognized as a single token. Compound tokens are identified independent of the character that creates the expression. The method automatically learns tokenization rules from a pre-tokenized corpus. The results obtained using the trainable system show that for Romanian and English a statistical significant improvement is obtained over a baseline system that tokenizes texts on every non-alphanumeric character.},
	language = {en},
	booktitle = {Proceedings of {LREC} 2008},
	publisher = {European Language Resources Association},
	author = {Frunza, Oana},
	year = {2008},
	file = {Frunza - A Trainable Tokenizer, solution for multilingual t.pdf:/home/stian/Zotero/storage/HWVJM7BV/Frunza - A Trainable Tokenizer, solution for multilingual t.pdf:application/pdf},
}

@inproceedings{graen_cutter_2018,
	address = {Winterthur, Switzerland},
	title = {Cutter: {A} universal multilingual tokenizer},
	volume = {2226},
	url = {http://ceur-ws.org/Vol-2226/},
	abstract = {Tokenization is the process of splitting running texts into minimal meaningful units. In writing systems where a space character is used for word separation, this blank character typically acts as token boundary. A simple tokenizer that only splits texts at space characters already achieves a notable accuracy, although it misses unmarked token boundaries and erroneously splits tokens that contain space characters.},
	language = {en},
	booktitle = {Proceedings of {SwissText} 2018},
	publisher = {ZHAW Zurich University of Applied Sciences},
	author = {Graën, Johannes and Bertamini, Mara and Volk, Martin},
	year = {2018},
	pages = {75--81},
	file = {Graën et al. - Cutter – a Universal Multilingual Tokenizer.pdf:/home/stian/Zotero/storage/EJBNA6PS/Graën et al. - Cutter – a Universal Multilingual Tokenizer.pdf:application/pdf},
}

@inproceedings{grefenstette_what_1994,
	address = {Budapest, Hungary},
	title = {What is a word, what is a sentence? {Problems} of tokenization},
	isbn = {963-8461-78-0},
	shorttitle = {What is a word, what is a sentence?},
	url = {https://www.semanticscholar.org/paper/What-is-a-word%2C-What-is-a-sentence-Problems-of-Grefenstette-Tapanainen/e727c7fd2bf3460a36934eae64c8c5716bc28980},
	abstract = {This discussion will cover the aspects of what is usually considered preprocessing of text in order to prepare it for some automated treatment, including the roles of tokenization, methods of tokenizing, grammars for recognizing acronyms, abbreviations, and regular expressions such as numbers and dates. Any linguistic treatment of freely occurring text must provide an answer to what is considered as a token. In arti cial languages, the de nition of what is considered as a token can be precisely and unambiguously de ned. Natural languages, on the other hand, display such a rich variety that there are many ways to decide upon what will be considered as a unit for a computational approach to text. Here we will discuss tokenization as a problem for computational lexicography. Our discussion will cover the aspects of what is usually considered preprocessing of text in order to prepare it for some automated treatment. We present the roles of tokenization, methods of tokenizing, grammars for recognizing acronyms, abbreviations, and regular expressions such as numbers and dates. We present the problems encountered and discuss the e ects of seemingly innocent choices.},
	language = {en},
	urldate = {2022-04-30},
	booktitle = {Proceedings of {COMPLEX} '94},
	publisher = {Hungarian Academy of Sciences},
	author = {Grefenstette, Gregory and Tapanainen, Pasi},
	year = {1994},
	pages = {79--87},
	file = {Grefenstette and Tapanainen - 1994 - What is a word, what is a sentence Problems of to.pdf:/home/stian/Zotero/storage/VUBD76HS/Grefenstette and Tapanainen - 1994 - What is a word, what is a sentence Problems of to.pdf:application/pdf},
}

@inproceedings{manning_stanford_2014,
	address = {Baltimore, Maryland},
	title = {The {Stanford} {CoreNLP} natural language processing toolkit},
	url = {https://aclanthology.org/P14-5010},
	doi = {10.3115/v1/P14-5010},
	abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
	urldate = {2022-04-30},
	booktitle = {Proceedings of 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
	month = jun,
	year = {2014},
	pages = {55--60},
	file = {Full Text PDF:/home/stian/Zotero/storage/4JGXSWBH/Manning et al. - 2014 - The Stanford CoreNLP Natural Language Processing T.pdf:application/pdf},
}

@inproceedings{nanni_ukparl_2018,
	address = {Miyazaki, Japan},
	title = {{UKParl}: {A} data set for topic detection with semantically annotated text},
	isbn = {979-10-95546-02-3},
	url = {http://lrec-conf.org/workshops/lrec2018/W2/summaries/6_W2.html},
	abstract = {We present a dataset created from the Hansard House of Commons archived debates of the UK parliament (2013-2016). The resource includes ﬁne-grained topic annotations at the document level and is enriched with additional semantic information such as the one provided by entity links. We assess the quality and usefulness of this corpus with two benchmarks on topic classiﬁcation and ranking.},
	language = {en},
	booktitle = {{ParlaCLARIN} 2018 {Workshop} {Proceedings}},
	publisher = {European Language Resources Association},
	author = {Nanni, Federico and Osman, Mahmoud and Cheng, Yi-Ru and Ponzetto, Simone Paolo and Dietz, Laura},
	year = {2018},
	file = {Nanni et al. - UKParl A Data Set for Topic Detection with Semant.pdf:/home/stian/Zotero/storage/B7SAGKFW/Nanni et al. - UKParl A Data Set for Topic Detection with Semant.pdf:application/pdf},
}

@inproceedings{pancur_slovparl_2018,
	address = {Miyazaki, Japan},
	title = {{SlovParl} 2.0: {The} collection of {Slovene} parliamentary debates from the period of secession},
	isbn = {979-10-95546-02-3},
	url = {http://lrec-conf.org/workshops/lrec2018/W2/summaries/4_W2.html},
	abstract = {The paper describes the process of acquisition, up-translation, encoding, and annotation of the collection of the parliamentary debates from the Assembly of the Republic of Slovenia from 1990-1992, covering the period before, during, and after Slovenia became an independent country in 1991. The entire collection, comprising 232 sessions, 58,813 speeches and 10.8 million words was uniformly encoded in accordance with the Text Encoding Initiative (TEI) Guidelines, using the TEI module for drama texts. The corpus contains extensive meta-data about the speakers, a typology of sessions etc. and structural and editorial annotations. The corpus was also converted to use the spoken corpus module of the TEI, and from this encoding automatically part-of-speech tagged and lemmatised. The corpus is maintained on GitHub and its major versions archived in the CLARIN.SI repository and is available for analysis under the CLARIN.SI concordancers, offering an invaluable resource for historians studying this watershed period of Slovenian history.},
	language = {en},
	booktitle = {{ParlaCLARIN} 2018 {Workshop} {Proceedings}},
	publisher = {European Language Resources Association},
	author = {Pančur, Andrej and Šorn, Mojca and Erjavec, Tomaž},
	year = {2018},
	pages = {7},
	file = {Pan - SlovParl 2.0 The Collection of Slovene Parliament.pdf:/home/stian/Zotero/storage/98EXPG4W/Pan - SlovParl 2.0 The Collection of Slovene Parliament.pdf:application/pdf},
}

@inproceedings{rodven-eide_swedish_2019,
	address = {Florence, Italy},
	title = {The {Swedish} {PoliGraph}: {A} semantic graph for argument mining of {Swedish} parliamentary data},
	copyright = {Creative Commons Attribution-ShareAlike 4.0 International License (CC-BY-SA)},
	shorttitle = {The {Swedish} {PoliGraph}},
	url = {https://aclanthology.org/W19-4506},
	doi = {10.18653/v1/W19-4506},
	abstract = {As part of a larger project on argument mining of Swedish parliamentary data, we have created a semantic graph that, together with named entity recognition and resolution (NER), should make it easier to establish connections between arguments in a given debate. The graph is essentially a semantic database that keeps track of Members of Parliament (MPs), in particular their presence in the parliament and activity in debates, but also party affiliation and participation in commissions. The hope is that the Swedish PoliGraph will enable us to perform named entity resolution on debates in the Swedish parliament with a high accuracy, with the aim of determining to whom an argument is directed.},
	urldate = {2022-04-30},
	booktitle = {Proceedings of the 6th {Workshop} on {Argument} {Mining}},
	publisher = {Association for Computational Linguistics},
	author = {Rødven-Eide, Stian},
	month = aug,
	year = {2019},
	pages = {52--57},
	file = {Full Text PDF:/home/stian/Zotero/storage/IF4PQXVC/Eide - 2019 - The Swedish PoliGraph A Semantic Graph for Argume.pdf:application/pdf},
}

@article{van_aggelen_debates_2017,
	title = {The debates of the {European} {Parliament} as {Linked} {Open} {Data}},
	volume = {8},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw227?resultNumber=2&totalResults=1641&start=0&q=parliament&resultsPageSize=10&rows=10},
	doi = {10.3233/SW-160227},
	abstract = {The European Parliament represents the citizens of the member states of the European Union (EU). The accounts of its meetings and related documents are open data, promoting transparency and accountability, and are used as source data by researchers.},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Semantic Web},
	author = {van Aggelen, Astrid and Hollink, Laura and Kemman, Max and Kleppe, Martijn and Beunders, Henri},
	year = {2017},
	note = {Publisher: IOS Press},
	pages = {271--281},
	file = {Full Text:/home/stian/Zotero/storage/WGQUHV2X/van Aggelen et al. - 2017 - The debates of the European Parliament as&nbsp\;Lin.pdf:application/pdf;Snapshot:/home/stian/Zotero/storage/7ZZYBXZQ/sw227.html:text/html},
}

@misc{hollink_talk_2017,
	title = {Talk of {Europe}: {The} debates of the {European} {Parliament} as {Linked} {Open} {Data}},
	copyright = {info:eu-repo/semantics/openAccess, License: http://creativecommons.org/publicdomain/zero/1.0},
	url = {https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:76464},
	abstract = {The Talk of Europe (TOE) project has created a Linked Open Dataset with data from and about the European Parliament (EP). The proceedings of the parliamentary debates as published by the EP, including all available translations, were converted to RDF and enriched with datasets of biographical and political information about the speakers. The ToE project officially ran from from 2014 to 2015 and has continued to make improved and updated versions of the data available online. The current dataset that is archived here contains all debates from summer 1999 until summer 2017. The dataset is documented in a publication: Astrid van Aggelen, Laura Hollink, Max Kemman, Martijn Kleppe, Henri Beunders. The debates of the European Parliament as Linked Open Data. Semantic Web, vol. 8, no. 2, pp. 271-281, 2017, IOS Press.},
	language = {en},
	urldate = {2022-04-30},
	publisher = {Data Archiving and Networked Services (DANS)},
	author = {Hollink, Laura and van Aggelen, Astrid and Beunders, Henri and Kleppe, Martijn and Kemman, Max and van Ossenbruggen, Jacco},
	year = {2017},
	doi = {10.17026/DANS-X62-EW3M},
	note = {Medium: RDF (turtle),application/x-cmdi+xml
Type: dataset},
	keywords = {European Parliament, FOS: Social sciences, Language and literature studies, Linked Open Data, Media sciences, Modern and contemporary history, multilingual data, Open Government Data, Social sciences, Temporal coverage: 1999-2017},
}

@article{hoyland_forum_2009,
	title = {Forum section: {An} automated database of the {European} parliament},
	volume = {10},
	issn = {1465-1165},
	shorttitle = {Forum section},
	url = {https://doi.org/10.1177/1465116508099764},
	doi = {10.1177/1465116508099764},
	language = {en},
	number = {1},
	urldate = {2022-04-30},
	journal = {European Union Politics},
	author = {Høyland, Bjørn and Sircar, Indraneel and Hix, Simon},
	month = mar,
	year = {2009},
	note = {Publisher: SAGE Publications},
	pages = {143--152},
	file = {Høyland et al. - 2009 - Forum section an automated database of the Europe.pdf:/home/stian/Zotero/storage/N8R2Y922/Høyland et al. - 2009 - Forum section an automated database of the Europe.pdf:application/pdf},
}

@article{lager_pengines_2014,
	title = {Pengines: {Web} logic programming made easy},
	volume = {14},
	issn = {1471-0684, 1475-3081},
	shorttitle = {Pengines},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/pengines-web-logic-programming-made-easy/4C43EAE4DBA200D589F70168D3C3C9B0},
	doi = {10.1017/S1471068414000192},
	abstract = {When developing a (web) interface for a deductive database, functionality required by the client is provided by means of HTTP handlers that wrap the logical data access predicates. These handlers are responsible for converting between client and server data representations and typically include options for paginating results. Designing the web accessible API is difficult because it is hard to predict the exact requirements of clients. Pengines changes this picture. The client provides a Prolog program that selects the required data by accessing the logical API of the server. The pengine infrastructure provides general mechanisms for converting Prolog data and handling Prolog non-determinism. The Pengines library is small (2000 lines Prolog, 150 lines JavaScript). It greatly simplifies defining an AJAX based client for a Prolog program and provides non-deterministic RPC between Prolog processes as well as interaction with Prolog engines similar to Paul Tarau's engines. Pengines are available as a standard package for SWI-Prolog 7.
                  1},
	language = {en},
	number = {4-5},
	urldate = {2022-04-30},
	journal = {Theory and Practice of Logic Programming},
	author = {Lager, Torbjörn and Wielemaker, Jan},
	month = jul,
	year = {2014},
	note = {Publisher: Cambridge University Press},
	keywords = {agent programming, distributed programming, query languages, web programming},
	pages = {539--552},
	file = {Snapshot:/home/stian/Zotero/storage/6EK773II/4C43EAE4DBA200D589F70168D3C3C9B0.html:text/html;Submitted Version:/home/stian/Zotero/storage/EH9CK8BS/Lager and Wielemaker - 2014 - Pengines Web Logic Programming Made Easy.pdf:application/pdf},
}

@article{lapponi_talk_2018,
	title = {The {Talk} of {Norway}: {A} richly annotated corpus of the {Norwegian} parliament, 1998–2016},
	volume = {52},
	issn = {1574-0218},
	shorttitle = {The {Talk} of {Norway}},
	url = {https://doi.org/10.1007/s10579-018-9411-5},
	doi = {10.1007/s10579-018-9411-5},
	abstract = {In this work we present the Talk of Norway (ToN) data set, a collection of Norwegian Parliament speeches from 1998 to 2016.Every speech is richly annotated with metadata harvested from different sources, and augmented with language type, sentence, token, lemma, part-of-speech, and morphological feature annotations. We also present a pilot study on party classification in the Norwegian Parliament, carried out in the context of a cross-faculty collaboration involving researchers from both Political Science and Computer Science. Our initial experiments demonstrate how the linguistic and institutional annotations in ToN can be used to gather insights on how different aspects of the political process affect classification.},
	language = {en},
	number = {3},
	urldate = {2022-04-30},
	journal = {Language Resources and Evaluation},
	author = {Lapponi, Emanuele and Søyland, Martin G. and Velldal, Erik and Oepen, Stephan},
	month = sep,
	year = {2018},
	keywords = {Computational political sciences, Computational social science, Language technology, Natural language processing, Parliamentary proceedings},
	pages = {873--893},
	file = {Full Text PDF:/home/stian/Zotero/storage/LX2SFE44/Lapponi et al. - 2018 - The Talk of Norway a richly annotated corpus of t.pdf:application/pdf},
}

@book{lehmann_semantic_1992,
	address = {Oxford; New York},
	title = {Semantic networks in artificial intelligence},
	volume = {2},
	isbn = {978-0-08-042012-7},
	abstract = {Semantic Networks are graphic structures used to represent concepts and knowledge in computers. Key uses include natural language understanding, information retrieval, machine vision, object-oriented analysis and dynamic control of combat aircraft. This major collection addresses every level of reader interested in the field of knowledge representation. Easy to read surveys of the main research families, most written by the founders, are followed by 25 widely varied articles on semantic networks and the conceptual structure of the world. Some extend ideas of philosopher Charles S Peirce 100 years ahead of his time. Others show connections to databases, lattice theory, semiotics, real-world ontology, graph-grammers, lexicography, relational algebras, property inheritance and semantic primitives. Hundreds of pictures show semantic networks as a visual language of thought.},
	language = {English},
	publisher = {Pergamon Press},
	author = {Lehmann, Fritz and Rodin, Ervin Y},
	year = {1992},
	note = {OCLC: 26391254},
}

@article{lehmann_dbpedia_2015,
	title = {{DBpedia}: {A} large-scale, multilingual knowledge base extracted from {Wikipedia}},
	volume = {6},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw134},
	doi = {10.3233/SW-140134},
	abstract = {The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on the Web using Semantic Web and Linked Data technologies. The project extracts knowledge from 111 different language editions of},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Semantic Web},
	author = {Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N. and Hellmann, Sebastian and Morsey, Mohamed and van Kleef, Patrick and Auer, Sören and Bizer, Christian},
	month = jan,
	year = {2015},
	note = {Publisher: IOS Press},
	pages = {167--195},
	file = {Lehmann et al. - 2015 - DBpedia – A large-scale, multilingual knowledge ba.pdf:/home/stian/Zotero/storage/4UFAGQ4I/Lehmann et al. - 2015 - DBpedia – A large-scale, multilingual knowledge ba.pdf:application/pdf;Snapshot:/home/stian/Zotero/storage/QCFIRMKV/sw134.html:text/html},
}

@article{lippi_argumentation_2016,
	title = {Argumentation mining: {State} of the art and emerging trends},
	volume = {16},
	issn = {1533-5399},
	shorttitle = {Argumentation mining},
	url = {https://doi.org/10.1145/2850417},
	doi = {10.1145/2850417},
	abstract = {Argumentation mining aims at automatically extracting structured arguments from unstructured textual documents. It has recently become a hot topic also due to its potential in processing information originating from the Web, and in particular from social media, in innovative ways. Recent advances in machine learning methods promise to enable breakthrough applications to social and economic sciences, policy making, and information technology: something that only a few years ago was unthinkable. In this survey article, we introduce argumentation models and methods, review existing systems and applications, and discuss challenges and perspectives of this exciting new research area.},
	number = {2},
	urldate = {2022-04-30},
	journal = {ACM Transactions on Internet Technology},
	author = {Lippi, Marco and Torroni, Paolo},
	month = mar,
	year = {2016},
	keywords = {Argumentation mining, artificial intelligence, computational linguistics, knowledge representation, machine learning, social media},
	pages = {10:1--10:25},
	file = {Submitted Version:/home/stian/Zotero/storage/34HUEM6H/Lippi and Torroni - 2016 - Argumentation Mining State of the Art and Emergin.pdf:application/pdf},
}
